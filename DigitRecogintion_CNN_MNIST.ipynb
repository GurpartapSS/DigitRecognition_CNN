{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the reuired packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import tensorflow\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adamax, RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "#initialise random number seed\n",
    "np.random.seed(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the train and test data\n",
    "train = pd.read_csv(\"input/train.csv\")\n",
    "test = pd.read_csv(\"input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store and remove the target for the train dataset \n",
    "Y_train = train[\"label\"]\n",
    "\n",
    "X_train = train.drop(labels = [\"label\"],axis = 1) \n",
    "\n",
    "#g = sns.countplot(Y_train)\n",
    "#Y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       784\n",
       "unique        1\n",
       "top       False\n",
       "freq        784\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for null or wrong values in the dataset \n",
    "\n",
    "X_train.isnull().any().describe()\n",
    "test.isnull().any().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalise the Data\n",
    "X_train = X_train / 255.0\n",
    "test = test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape data to get 3-D image\n",
    "X_train = X_train.values.reshape(-1,28,28,1)\n",
    "test = test.values.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = to_categorical(Y_train, num_classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data for training and validation\n",
    "random_seed = 2\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = plt.imshow(X_train[0][:,:,0])\n",
    "#X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               803072    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 870,634\n",
      "Trainable params: 870,634\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#set the CNN model\n",
    "#model summary at the bottom \n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', activation ='relu', input_shape = (28,28,1)))\n",
    "#model.add(BatchNormalization(axis=3))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#model.add(BatchNormalization(axis=3))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "#model.add(BatchNormalization(axis=3))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation = \"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define and complie Optimizer\n",
    "\n",
    "optimizer = Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "#optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MiniConda\\envs\\tensorflow\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:355: UserWarning: This ImageDataGenerator specifies `samplewise_std_normalization`, which overrides setting of `samplewise_center`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    }
   ],
   "source": [
    "#Data Augmentation to improve accuracy\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=True,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0803 16:47:08.024933 21120 deprecation.py:323] From D:\\MiniConda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1251: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0803 16:47:09.095205 21120 deprecation.py:323] From D:\\MiniConda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py:460: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Apply a constraint manually following the optimizer update step.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295/295 [==============================] - 113s 383ms/step - loss: 0.5402 - accuracy: 0.8237 - val_loss: 0.4723 - val_accuracy: 0.9779\n",
      "Epoch 2/30\n",
      "295/295 [==============================] - 111s 375ms/step - loss: 0.1866 - accuracy: 0.9450 - val_loss: 0.3439 - val_accuracy: 0.9781\n",
      "Epoch 3/30\n",
      "295/295 [==============================] - 111s 375ms/step - loss: 0.1361 - accuracy: 0.9594 - val_loss: 0.3223 - val_accuracy: 0.9817\n",
      "Epoch 4/30\n",
      "295/295 [==============================] - 106s 359ms/step - loss: 0.1078 - accuracy: 0.9674 - val_loss: 0.2670 - val_accuracy: 0.9886\n",
      "Epoch 5/30\n",
      "295/295 [==============================] - 107s 363ms/step - loss: 0.0944 - accuracy: 0.9717 - val_loss: 0.2412 - val_accuracy: 0.9883\n",
      "Epoch 6/30\n",
      "295/295 [==============================] - 108s 365ms/step - loss: 0.0883 - accuracy: 0.9734 - val_loss: 0.2481 - val_accuracy: 0.9910\n",
      "Epoch 7/30\n",
      "295/295 [==============================] - 111s 375ms/step - loss: 0.0796 - accuracy: 0.9763 - val_loss: 0.2280 - val_accuracy: 0.9910\n",
      "Epoch 8/30\n",
      "295/295 [==============================] - 110s 374ms/step - loss: 0.0753 - accuracy: 0.9778 - val_loss: 0.2231 - val_accuracy: 0.9900\n",
      "Epoch 9/30\n",
      "295/295 [==============================] - 108s 365ms/step - loss: 0.0651 - accuracy: 0.9807 - val_loss: 0.1890 - val_accuracy: 0.9921\n",
      "Epoch 10/30\n",
      "295/295 [==============================] - 107s 362ms/step - loss: 0.0626 - accuracy: 0.9817 - val_loss: 0.1667 - val_accuracy: 0.9902\n",
      "Epoch 11/30\n",
      "295/295 [==============================] - 104s 353ms/step - loss: 0.0604 - accuracy: 0.9814 - val_loss: 0.2060 - val_accuracy: 0.9921\n",
      "Epoch 12/30\n",
      "295/295 [==============================] - 107s 364ms/step - loss: 0.0546 - accuracy: 0.9836 - val_loss: 0.1555 - val_accuracy: 0.9931\n",
      "Epoch 13/30\n",
      "295/295 [==============================] - 107s 364ms/step - loss: 0.0562 - accuracy: 0.9839 - val_loss: 0.2303 - val_accuracy: 0.9926\n",
      "Epoch 14/30\n",
      "295/295 [==============================] - 107s 362ms/step - loss: 0.0507 - accuracy: 0.9839 - val_loss: 0.1882 - val_accuracy: 0.9924\n",
      "Epoch 15/30\n",
      "295/295 [==============================] - 108s 367ms/step - loss: 0.0499 - accuracy: 0.9845 - val_loss: 0.1914 - val_accuracy: 0.9926\n",
      "Epoch 16/30\n",
      "295/295 [==============================] - 110s 373ms/step - loss: 0.0491 - accuracy: 0.9857 - val_loss: 0.1660 - val_accuracy: 0.9926\n",
      "Epoch 17/30\n",
      "295/295 [==============================] - 110s 374ms/step - loss: 0.0465 - accuracy: 0.9860 - val_loss: 0.1629 - val_accuracy: 0.9919\n",
      "Epoch 18/30\n",
      "295/295 [==============================] - 110s 373ms/step - loss: 0.0461 - accuracy: 0.9855 - val_loss: 0.1941 - val_accuracy: 0.9900\n",
      "Epoch 19/30\n",
      "295/295 [==============================] - 106s 359ms/step - loss: 0.0437 - accuracy: 0.9872 - val_loss: 0.2047 - val_accuracy: 0.9907\n",
      "Epoch 20/30\n",
      "295/295 [==============================] - 103s 349ms/step - loss: 0.0420 - accuracy: 0.9870 - val_loss: 0.2070 - val_accuracy: 0.9924\n",
      "Epoch 21/30\n",
      "295/295 [==============================] - 110s 373ms/step - loss: 0.0403 - accuracy: 0.9876 - val_loss: 0.1912 - val_accuracy: 0.9936\n",
      "Epoch 22/30\n",
      "295/295 [==============================] - 107s 362ms/step - loss: 0.0373 - accuracy: 0.9886 - val_loss: 0.1961 - val_accuracy: 0.9929\n",
      "Epoch 23/30\n",
      "295/295 [==============================] - 102s 347ms/step - loss: 0.0369 - accuracy: 0.9890 - val_loss: 0.1129 - val_accuracy: 0.9938\n",
      "Epoch 24/30\n",
      "295/295 [==============================] - 107s 361ms/step - loss: 0.0374 - accuracy: 0.9894 - val_loss: 0.1862 - val_accuracy: 0.9938\n",
      "Epoch 25/30\n",
      "295/295 [==============================] - 103s 349ms/step - loss: 0.0359 - accuracy: 0.9895 - val_loss: 0.1392 - val_accuracy: 0.9938\n",
      "Epoch 26/30\n",
      "295/295 [==============================] - 1585s 5s/step - loss: 0.0345 - accuracy: 0.9891 - val_loss: 0.1567 - val_accuracy: 0.9940\n",
      "Epoch 27/30\n",
      "295/295 [==============================] - 93s 314ms/step - loss: 0.0336 - accuracy: 0.9902 - val_loss: 0.1413 - val_accuracy: 0.9933\n",
      "Epoch 28/30\n",
      "295/295 [==============================] - 108s 367ms/step - loss: 0.0378 - accuracy: 0.9889 - val_loss: 0.2346 - val_accuracy: 0.9895\n",
      "Epoch 29/30\n",
      "295/295 [==============================] - 109s 369ms/step - loss: 0.0334 - accuracy: 0.9899 - val_loss: 0.1432 - val_accuracy: 0.9893\n",
      "Epoch 30/30\n",
      "295/295 [==============================] - 109s 370ms/step - loss: 0.0325 - accuracy: 0.9901 - val_loss: 0.2267 - val_accuracy: 0.9910\n"
     ]
    }
   ],
   "source": [
    "#Fit the model, validating the model alongside.\n",
    "#tap_model is model history, can be used to plot loss and accuracy curves for training and validation \n",
    "tap_model = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=128),epochs = 30, validation_data = (X_val,Y_val),\n",
    "                              verbose = 1, steps_per_epoch=X_train.shape[0] // 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4200/4200 [==============================] - 4s 846us/sample - loss: 0.2268 - accuracy: 0.9910\n",
      "Test loss: 0.226827704622632\n",
      "Test accuracy: 0.9909524\n"
     ]
    }
   ],
   "source": [
    "#Optional, if data not validated above\n",
    "#score = model.evaluate(X_val, Y_val, verbose=1)\n",
    "#print('Test loss:', score[0])\n",
    "#print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict results, test data from Kaggle \n",
    "predictions = model.predict_classes(test, verbose=0)\n",
    "\n",
    "submissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n",
    "                         \"Label\": predictions})\n",
    "submissions.to_csv(\"DR.csv\", index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
